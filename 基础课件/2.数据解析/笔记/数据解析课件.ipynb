{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据解析\n",
    "- 正则\n",
    "- bs4\n",
    "- xpath\n",
    "- pyquery（自学）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则解析\n",
    "    单字符：\n",
    "        . : 除换行以外所有字符\n",
    "        [] ：[aoe] [a-w] 匹配集合中任意一个字符\n",
    "        \\d ：数字  [0-9]\n",
    "        \\D : 非数字\n",
    "        \\w ：数字、字母、下划线、中文\n",
    "        \\W : 非\\w\n",
    "        \\s ：所有的空白字符包,括空格、制表符、换页符等等。等价于 [ \\f\\n\\r\\t\\v]。\n",
    "        \\S : 非空白\n",
    "    数量修饰：\n",
    "        * : 任意多次  >=0\n",
    "        + : 至少1次   >=1\n",
    "        ? : 可有可无  0次或者1次\n",
    "        {m} ：固定m次 hello{3,}\n",
    "        {m,} ：至少m次\n",
    "        {m,n} ：m-n次\n",
    "    边界：\n",
    "        $ : 以某某结尾 \n",
    "        ^ : 以某某开头\n",
    "    分组：\n",
    "        (ab)  \n",
    "    贪婪模式： .*\n",
    "    非贪婪（惰性）模式： .*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用正则进行图片数据的批量解析爬取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如何爬取图片数据？\n",
    "    - 方式1：基于requests\n",
    "    - 方式2：基于urllib\n",
    "        - urllib模块作用和requests模块一样，都是基于网络请求的模块。\n",
    "            - 当requests问世后就迅速的替代了urllib模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#方式1：\n",
    "img_url = 'https://ss1.bdstatic.com/70cFuXSh_Q1YnxGkpoWK1HF6hhy/it/u=1312059974,1893880587&fm=11&gp=0.jpg'\n",
    "response = requests.get(url=img_url,headers=headers)\n",
    "img_data = response.content #content返回的是二进制形式的响应数据\n",
    "with open('1.jpg','wb') as fp:\n",
    "    fp.write(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./2.jpg', <http.client.HTTPMessage at 0x10486a5c0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#方式2：\n",
    "img_url = 'https://ss1.bdstatic.com/70cFuXSh_Q1YnxGkpoWK1HF6hhy/it/u=1312059974,1893880587&fm=11&gp=0.jpg'\n",
    "#可以直接对url发起请求且进行持久化存储\n",
    "urllib.request.urlretrieve(img_url,'./2.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 上述两种爬取图片的操作不同之处是什么？\n",
    "    - 使用urllib的方式爬取图片无法进行UA伪装，而requests的方式可以。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 需求：爬取校花网中的图片数据\n",
    "    - www.521609.com\n",
    "    - 操作：需要将每一张图片的地址解析出来，然后对图片地址发起请求即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 分析浏览器开发者工具中Elements和network这两个选项卡对应的页面源码数据有何不同之处？\n",
    "    - Elements中包含的显示的页面源码数据为当前页面所有的数据加载完毕后对应的完整的页面源码数据（包含了动态加载数据）\n",
    "    - network中显示的页面源码数据仅仅为某一个单独的请求对应的响应数据（不包含动态加载数据）\n",
    "    - 结论：如果在进行数据解析的时候，一定是需要对页面布局进行分析，如果当前网站没有动态加载的数据就可以直接使用Elements对页面布局进行分析，否则只可以使用network对页面数据进行分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImgLibs/11046303404-1-lp.jpg 下载成功！！！\n",
      "ImgLibs/1152K12363-1-lp.jpg 下载成功！！！\n",
      "ImgLibs/10I4561000-1-lp.jpg 下载成功！！！\n",
      "ImgLibs/116234L914-1-lp.jpg 下载成功！！！\n",
      "ImgLibs/12236009605-1-lp.jpg 下载成功！！！\n",
      "ImgLibs/11I24425C-1-lp.jpg 下载成功！！！\n",
      "ImgLibs/1093002H94-1-lp.jpg 下载成功！！！\n",
      "ImgLibs/10910261217-1-lp.jpg 下载成功！！！\n",
      "ImgLibs/22111113364.jpg 下载成功！！！\n",
      "ImgLibs/13554311594.jpg 下载成功！！！\n",
      "ImgLibs/20052013192.jpg 下载成功！！！\n",
      "ImgLibs/1_194614R3.jpg 下载成功！！！\n",
      "ImgLibs/13545GI62.jpg 下载成功！！！\n",
      "ImgLibs/000F41S33.jpg 下载成功！！！\n",
      "ImgLibs/09355G45c.jpg 下载成功！！！\n",
      "ImgLibs/00343314205.jpg 下载成功！！！\n"
     ]
    }
   ],
   "source": [
    "dirName = 'ImgLibs'\n",
    "if not os.path.exists(dirName):\n",
    "    os.mkdir(dirName)\n",
    "    \n",
    "#1.捕获到当前首页的页面源码数据\n",
    "url = 'http://www.521609.com/qingchunmeinv/'\n",
    "page_text = requests.get(url=url,headers=headers).text\n",
    "\n",
    "#2.从当前获取的页面源码数据中解析出图片地址\n",
    "ex = '<li>.*?<img src=\"(.*?)\" width=.*?</li>'\n",
    "img_src_list = re.findall(ex,page_text,re.S)\n",
    "for src in img_src_list:\n",
    "    src = 'http://www.521609.com'+src\n",
    "    imgPath = dirName+'/'+src.split('/')[-1]\n",
    "    urllib.request.urlretrieve(src,imgPath)\n",
    "    print(imgPath,'下载成功！！！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据解析的作用？\n",
    "    - 用来实现聚焦爬虫。\n",
    "- 网页中显示的数据都是存储在哪里？\n",
    "    - 都是存储在html的标签中或者是标签的属性中\n",
    "- 数据解析的通用原理是什么？\n",
    "    - 指定标签的定位\n",
    "    - 取出标签中存储的数据或者标签属性中的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bs4\n",
    "- bs4解析原理\n",
    "    - 实例化一个BeautifulSoup的对象，且将待解析的页面源码数据加载到该对象中\n",
    "    - 调用BeautifulSoup对象中相关方法或者属性进行标签定位和文本数据的提取\n",
    "- 环境安装：\n",
    "    - pip install lxml #解析器\n",
    "    - pip install bs4\n",
    "- BeautifulSoup对象的实例化：\n",
    "    - BeautifulSoup(fp,'lxml')：用来将本地存储的html文档中的数据进行解析\n",
    "    - BeautifulSoup(page_text，’lxml‘)：用来将互联网上请求到的页面源码数据进行解析\n",
    "- 标签定位：\n",
    "    - soup.tagName：只可以定位到第一次出现的tagName标签\n",
    "    - soup.find('tagName',attrName='value'):属性定位\n",
    "    - soup.findAll:跟find一样用作属性定位，只不过findAll返回的是列表\n",
    "    - soup.select('选择器'):选择器定位\n",
    "        - 类选择器\n",
    "        - id选择器\n",
    "        - 层级选择\n",
    "            - 大于号:表示一个层级\n",
    "            - 空格：表示多个层级\n",
    "- 取数据\n",
    "    - .text：返回的是该标签下所有的文本内容\n",
    "    - .string:返回的是该标签直系的文本内容\n",
    "- 取属性：\n",
    "    - tag['attrName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0xa7 in position 161: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21176/909537316.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./test.html'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'song'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\work\\python\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'read'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m        \u001b[1;31m# It's a file-type object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m             \u001b[0mmarkup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         elif len(markup) <= 256 and (\n\u001b[0;32m    311\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;34mb'<'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xa7 in position 161: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "fp = open('./test.html','r')\n",
    "soup = BeautifulSoup(fp,'lxml')\n",
    "soup.p\n",
    "soup.find('div',class_='song')\n",
    "soup.find('a',id='feng')\n",
    "soup.findAll('a',id='feng')\n",
    "soup.select('.tang')\n",
    "soup.select('.tang li')\n",
    "a_tag = soup.find('a',id='feng')\n",
    "a_tag['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n李清照\\n王安石\\n苏轼\\n柳宗元\\n\\nthis is span\\n\\t\\t宋朝是最强大的王朝，不是军队的强大，而是经济很强大，国民都很有钱\\n总为浮云能蔽日,长安不见使人愁\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_tag = soup.find('div',class_='song')\n",
    "div_tag.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 爬取三国全篇内容：http://www.shicimingju.com/book/sanguoyanyi.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¬¬ä¸åÂ·å®´æ¡å­è±ªæ°ä¸ç»ä¹  æ©é»å·¾è±éé¦ç«å 保存成功！！！\n",
      "ç¬¬äºåÂ·å¼ ç¿¼å¾·æé­ç£é®    ä½å½è",
      "è°è¯å®¦ç« 保存成功！！！\n",
      "ç¬¬ä¸åÂ·è®®æ¸©æè£åå±ä¸å  é¦éç æèè¯´åå¸ 保存成功！！！\n",
      "ç¬¬ååÂ·åºæ±å¸éçè·µä½    è°è£è´¼å­å¾·ç®å 保存成功！！！\n",
      "ç¬¬äºåÂ·åç«è¯è¯¸éåºæ¹å",
      "¬  ç ´å",
      "³å",
      "µä¸è±æåå¸ 保存成功！！！\n",
      "ç¬¬å",
      "­åÂ·çééè£åè¡å¶    å¿ççºå­åèçº¦ 保存成功！！！\n",
      "ç¬¬ä¸åÂ·è¢ç»ç£æ²³æå",
      "¬å­    å­åè·¨æ±å»åè¡¨ 保存成功！！！\n",
      "ç¬¬å",
      "«åÂ·çå¸å¾å·§ä½¿è¿ç¯è®¡  è£å¤ªå¸å¤§é¹å¤ä»ªäº­ 保存成功！！！\n",
      "ç¬¬ä¹åÂ·é¤æ´å¶åå¸å©å¸å¾  ç¯é¿å®æåå¬è´¾è¯© 保存成功！！！\n",
      "ç¬¬ä¸ååÂ·å¤çå®¤é©¬è",
      "¾ä¸¾ä¹    æ¥ç¶ä»æ¹æå",
      "´å¸ 保存成功！！！\n",
      "ç¬¬åä¸åÂ·åçååæµ·æå­è  åæ¸©ä¾¯æ¿®é³ç ´æ¹æ 保存成功！！！\n",
      "ç¬¬åäºåÂ·é¶æ­ç¥ä¸è®©å¾å·    æ¹å­å¾·å¤§æåå¸ 保存成功！！！\n",
      "ç¬¬åä¸åÂ·æåé­æ±å¤§äº¤å",
      "µ  æ¨å¥è£æ¿åæé©¾ 保存成功！！！\n",
      "ç¬¬åååÂ·æ¹å­å¾·ç§»é©¾å¹¸è®¸é½  åå¥å",
      "ä¹å¤è¢­å¾é¡ 保存成功！！！\n",
      "ç¬¬åäºåÂ·å¤ªå²æ",
      "é",
      "£æå°é¸ç  å­ä¼¯ç¬¦å¤§æä¸¥ç½è 保存成功！！！\n",
      "ç¬¬åå",
      "­åÂ·åå¥å",
      "å°æè¾é¨    æ¹å­å¾·è´¥å¸æ·¯æ°´ 保存成功！！！\n",
      "ç¬¬åä¸åÂ·è¢å",
      "¬è·¯å¤§èµ·ä¸å    æ¹å­å¾·ä¼åä¸å° 保存成功！！！\n",
      "ç¬¬åå",
      "«åÂ·è´¾æåææå³è    å¤ä¾¯ææç¢åç 保存成功！！！\n",
      "ç¬¬åä¹åÂ·ä¸é³åæ¹æéå",
      "µ    ç½é¨æ¥¼åå¸æ®å½ 保存成功！！！\n",
      "ç¬¬äºååÂ·æ¹é¿çè®¸ç°æå´    è£å½è",
      "å",
      "éåè¯ 保存成功！！！\n",
      "ç¬¬äºåä¸åÂ·æ¹æç",
      "®é",
      "è®ºè±é  å",
      "³å",
      "¬èµåæ©è½¦è 保存成功！！！\n",
      "ç¬¬äºåäºåÂ·è¢æ¹åèµ·é©¬æ­¥ä¸å  å",
      "³å¼ å",
      "±æçåäºå° 保存成功！！！\n",
      "ç¬¬äºåä¸åÂ·ç¥¢æ­£å¹³è£¸è¡£éªè´¼    åå¤ªå»ä¸æ¯é­å 保存成功！！！\n",
      "ç¬¬äºåååÂ·å½è´¼è¡å¶æè´µå¦    çåè´¥èµ°æè¢ç» 保存成功！！！\n",
      "ç¬¬äºåäºåÂ·å±¯åå±±å",
      "³å",
      "¬çº¦ä¸äº  æç½é©¬æ¹æè§£éå´ 保存成功！！！\n",
      "ç¬¬äºåå",
      "­åÂ·è¢æ¬åè´¥å",
      "µæå°    å",
      "³äºé¿æå°å°é 保存成功！！！\n",
      "ç¬¬äºåä¸åÂ·ç¾é«¯å",
      "¬åéèµ°åéª  æ±å¯¿ä¾¯äºå",
      "³æ©å",
      "­å° 保存成功！！！\n",
      "ç¬¬äºåå",
      "«åÂ·æ©è¡é³å",
      "å¼éç    ä¼å¤åä¸»è£èä¹ 保存成功！！！\n",
      "ç¬¬äºåä¹åÂ·å°é¸çææ©äºå    ç¢§ç¼å¿åé¢æ±ä¸ 保存成功！！！\n",
      "ç¬¬ä¸ååÂ·æå®æ¸¡æ¬åè´¥ç»©  å«ä¹å·¢å­å¾·ç§ç²® 保存成功！！！\n",
      "ç¬¬ä¸åä¸åÂ·æ¹æä»äº­ç ´æ¬å    çå¾·èå·ä¾åè¡¨ 保存成功！！！\n",
      "ç¬¬ä¸åäºåÂ·å¤ºåå·è¢å°äºé    å³æ¼³æ²³è®¸æ¸ç®è®¡ 保存成功！！！\n",
      "ç¬¬ä¸åä¸åÂ·æ¹ä¸ä¹ä¹±çº³çæ°    é­åéè®¡å®è¾½ä¸ 保存成功！！！\n",
      "ç¬¬ä¸åååÂ·è¡å¤«äººéå±å¬å¯è¯­  åçåè·é©¬è¿æªæºª 保存成功！！！\n",
      "ç¬¬ä¸åäºåÂ·çå¾·åæ¼³é¢éæ²§    åç¦æ°ééè±ä¸» 保存成功！！！\n",
      "ç¬¬ä¸åå",
      "­åÂ·çå¾·ç¨è®¡è¢­æ¨å    å",
      "ç´èµ°é©¬èè¯¸è 保存成功！！！\n",
      "ç¬¬ä¸åä¸åÂ·å¸é©¬å¾½åèåå£«    åçå¾·ä¸é¡¾èåº 保存成功！！！\n",
      "ç¬¬ä¸åå",
      "«åÂ·å®ä¸åéä¸­å³ç­    æé¿æ±å­æ°æ¥ä» 保存成功！！！\n",
      "ç¬¬ä¸åä¹åÂ·èå·åå",
      "¬å­ä¸æ±è®¡  åæå¡åå¸åç¨å",
      "µ 保存成功！！！\n",
      "ç¬¬åååÂ·è¡å¤«äººè®®ç®èå·    è¯¸èäº®ç«ç§æ°é 保存成功！！！\n",
      "ç¬¬ååä¸åÂ·åçå¾·æºæ°æ¸¡æ±    èµµå­é¾åéªæä¸» 保存成功！！！\n",
      "ç¬¬ååäºåÂ·å¼ ç¿¼å¾·å¤§é¹é¿åæ¡¥  åè±«å·è´¥èµ°æ±æ´¥å£ 保存成功！！！\n",
      "ç¬¬ååä¸åÂ·è¯¸èäº®èæç¾¤å    é²å­æ¬åæä¼è®® 保存成功！！！\n",
      "ç¬¬ååååÂ·å­æç¨æºæ¿å¨ç    å­æå³è®¡ç ´æ¹æ 保存成功！！！\n",
      "ç¬¬ååäºåÂ·ä¸æ±å£æ¹ææå",
      "µ    ç¾¤è±ä¼èå¹²ä¸­è®¡ 保存成功！！！\n",
      "ç¬¬ååå",
      "­åÂ·ç¨å¥è°å­æåç®­    ç®å¯è®¡é»çåå 保存成功！！！\n",
      "ç¬¬ååä¸åÂ·éæ³½å¯ç®è¯éä¹¦    åºç»å·§æè¿ç¯è®¡ 保存成功！！！\n",
      "ç¬¬ååå",
      "«åÂ·å®´é¿æ±æ¹æèµè¯    éæè¹ååç¨æ­¦ 保存成功！！！\n",
      "ç¬¬ååä¹åÂ·ä¸æåè¯¸èç¥­é£    ä¸æ±å£å¨ççºµç« 保存成功！！！\n",
      "ç¬¬äºååÂ·è¯¸èäº®æºç®åå®¹    å",
      "³äºé¿ä¹éæ¹æ 保存成功！！！\n",
      "ç¬¬äºåä¸åÂ·æ¹ä»å¤§æä¸å´å",
      "µ    å­æä¸æ°å¨å",
      "¬ç¾ 保存成功！！！\n",
      "ç¬¬äºåäºåÂ·è¯¸èäº®æºè¾é²è    èµµå­é¾è®¡åæ¡é³ 保存成功！！！\n",
      "ç¬¬äºåä¸åÂ·å",
      "³äºé¿ä¹éé»æ±å  å­ä»²è°å¤§æå¼ æè¿ 保存成功！！！\n",
      "ç¬¬äºåååÂ·å´å½å¤ªä½å¯ºçæ°é  åçåæ´æ¿ç»­ä½³å¶ 保存成功！！！\n",
      "ç¬¬äºåäºåÂ·çå¾·æºæ¿å­å¤«äºº    å­æäºæ°å¨å",
      "¬ç¾ 保存成功！！！\n",
      "ç¬¬äºåå",
      "­åÂ·æ¹æå¤§å®´ééå°    å­æä¸æ°å¨å",
      "¬ç¾ 保存成功！！！\n",
      "ç¬¬äºåä¸åÂ·æ´æ¡å£å§é¾åä¸§    èé³å¿å¤éçäº 保存成功！！！\n",
      "ç¬¬äºåå",
      "«åÂ·é©¬å­èµ·å",
      "´å",
      "µéªæ¨    æ¹é¿çå²é¡»å¼è¢ 保存成功！！！\n",
      "ç¬¬äºåä¹åÂ·è®¸è¯¸è£¸è¡£æé©¬è¶",
      "    æ¹ææ¹ä¹¦é®é©é 保存成功！！！\n",
      "ç¬¬å",
      "­ååÂ·å¼ æ°¸å¹´åé¾æ¨ä¿®    åºå£«å",
      "è®®åè¥¿è 保存成功！！！\n",
      "ç¬¬å",
      "­åä¸åÂ·èµµäºæªæ±å¤ºé¿æ    å­æéä¹¦éèç 保存成功！！！\n",
      "ç¬¬å",
      "­åäºåÂ·åæ¶ªå",
      "³æ¨é«æé¦    æ»éåé»é­äºå 保存成功！！！\n",
      "ç¬¬å",
      "­åä¸åÂ·è¯¸èäº®çå­åºç»    å¼ ç¿¼å¾·ä¹éä¸¥é¢ 保存成功！！！\n",
      "ç¬¬å",
      "­åååÂ·å­æå®è®¡æå¼ ä»»    æ¨éåå",
      "µç ´é©¬è¶",
      " 保存成功！！！\n",
      "ç¬¬å",
      "­åäºåÂ·é©¬è¶",
      "å¤§æè­èå",
      "³    åå¤èªé¢çå·ç§ 保存成功！！！\n",
      "ç¬¬å",
      "­åå",
      "­åÂ·å",
      "³äºé¿ååèµ´ä¼    ä¼çåä¸ºå½æç 保存成功！！！\n",
      "ç¬¬å",
      "­åä¸åÂ·æ¹æå¹³å®æ±ä¸­å°    å¼ è¾½å¨ééé¥æ´¥ 保存成功！！！\n",
      "ç¬¬å",
      "­åå",
      "«åÂ·çå®ç¾éªå«é­è¥    å·¦æ",
      "æ·æ¯ææ¹æ 保存成功！！！\n",
      "ç¬¬å",
      "­åä¹åÂ·åå¨æç®¡è¾ç¥æº    è®¨æ±è´¼äºè£æ­»è 保存成功！！！\n",
      "ç¬¬ä¸ååÂ·çå¼ é£æºåç¦å£é  èé»å¿ è®¡å¤ºå¤©è¡å±± 保存成功！！！\n",
      "ç¬¬ä¸åä¸åÂ·å å¯¹å±±é»å¿ é¸å¾",
      "å³  æ®æ±æ°´èµµäºå¯¡èä¼ 保存成功！！！\n",
      "ç¬¬ä¸åäºåÂ·è¯¸èäº®æºåæ±ä¸­    æ¹é¿çå",
      "µéæè°· 保存成功！！！\n",
      "ç¬¬ä¸åä¸åÂ·çå¾·è¿ä½æ±ä¸­ç    äºé¿æ»æè¥é³é¡ 保存成功！！！\n",
      "ç¬¬ä¸åååÂ·åºä»¤ææ¬æ¦å³æ­»æ  å",
      "³äºé¿æ¾æ°´æ·¹ä¸å 保存成功！！！\n",
      "ç¬¬ä¸åäºåÂ·å",
      "³äºé¿å®éª¨çæ¯    åå­æç½è¡£æ¸¡æ± 保存成功！！！\n",
      "ç¬¬ä¸åå",
      "­åÂ·å¾å",
      "¬æå¤§ææ²æ°´    å",
      "³äºé¿è´¥èµ°éº¦å 保存成功！！！\n",
      "ç¬¬ä¸åä¸åÂ·çæ³å±±å",
      "³å",
      "¬æ¾å£    æ´é³åæ¹ææç¥ 保存成功！！！\n",
      "ç¬¬ä¸åå",
      "«åÂ·æ²»é£ç¾ç¥å»èº«æ­»    ä¼ éå½å¥¸éæ°ç» 保存成功！！！\n",
      "ç¬¬ä¸åä¹åÂ·å",
      "é¼å¼æ¹æ¤èµè¯    ä¾é·ååå°ä¼æ³ 保存成功！！！\n",
      "ç¬¬å",
      "«ååÂ·æ¹ä¸åºå¸ç¯¡çå    æ±çæ­£ä½ç»­å¤§ç» 保存成功！！！\n",
      "ç¬¬å",
      "«åä¸åÂ·æ¥å",
      "ä»å¼ é£éå®³    éªå¼æ¨å",
      "ä¸»å",
      "´å",
      "µ 保存成功！！！\n",
      "ç¬¬å",
      "«åäºåÂ·å­æéé­åä¹é¡    å",
      "ä¸»å¾å´èµå",
      "­å 保存成功！！！\n",
      "ç¬¬å",
      "«åä¸åÂ·æçäº­å",
      "ä¸»å¾ä»äºº  å®æ±å£ä¹¦çæå¤§å° 保存成功！！！\n",
      "ç¬¬å",
      "«åååÂ·ééè¥ç§ä¸ç¾é    å­æå·§å¸å",
      "«éµå¾ 保存成功！！！\n",
      "ç¬¬å",
      "«åäºåÂ·åå",
      "ä¸»éè¯æå­¤å¿  è¯¸èäº®å®å±",
      "å¹³äºè·¯ 保存成功！！！\n",
      "ç¬¬å",
      "«åå",
      "­åÂ·é¾å¼ æ¸©ç§¦å®éå¤©è¾©  ç ´æ¹ä¸å¾çç¨ç«æ» 保存成功！！！\n",
      "ç¬¬å",
      "«åä¸åÂ·å¾åå¯ä¸ç¸å¤§å",
      "´å¸  æå¤©å",
      "µè®çååæ§ 保存成功！！！\n",
      "ç¬¬å",
      "«åå",
      "«åÂ·æ¸¡æ³¸æ°´åç¼çªç    è¯è¯éä¸æå­è· 保存成功！！！\n",
      "ç¬¬å",
      "«åä¹åÂ·æ­¦ä¹¡ä¾¯åçªç¨è®¡    åè®çäºæ¬¡é­æ 保存成功！！！\n",
      "ç¬¬ä¹ååÂ·é©±å·¨åå",
      "­ç ´è®å",
      "µ    ç§è¤ç²ä¸æå­è· 保存成功！！！\n",
      "ç¬¬ä¹åä¸åÂ·ç¥­æ³¸æ°´æ±ç¸ç­å¸    ä¼ä¸­åæ­¦ä¾¯ä¸è¡¨ 保存成功！！！\n",
      "ç¬¬ä¹åäºåÂ·èµµå­é¾åæ©äºå°    è¯¸èäº®æºåä¸å 保存成功！！！\n",
      "ç¬¬ä¹åä¸åÂ·å§ä¼¯çº¦å½éå­æ    æ­¦ä¹¡ä¾¯éªæ­»çæ 保存成功！！！\n",
      "ç¬¬ä¹åååÂ·è¯¸èäº®ä¹éªç ´ç¾å",
      "µ  å¸é©¬æ¿å",
      "æ¥æå­è¾¾ 保存成功！！！\n",
      "ç¬¬ä¹åäºåÂ·é©¬è°¡æè°å¤±è¡äº­    æ­¦ä¾¯å¼¹ç´éä»²è¾¾ 保存成功！！！\n",
      "ç¬¬ä¹åå",
      "­åÂ·å­ææ¥æ³ªæ©é©¬è°¡    å¨é²æ­åèµæ¹ä¼ 保存成功！！！\n",
      "ç¬¬ä¹åä¸åÂ·è®¨é­å½æ­¦ä¾¯åä¸è¡¨  ç ´æ¹å",
      "µå§ç»´è¯ç®ä¹¦ 保存成功！！！\n",
      "ç¬¬ä¹åå",
      "«åÂ·è¿½æ±åçååè¯    è¢­éä»æ­¦ä¾¯åè 保存成功！！！\n",
      "ç¬¬ä¹åä¹åÂ·è¯¸èäº®å¤§ç ´é­å",
      "µ    å¸é©¬æ¿å",
      "¥å¯è¥¿è 保存成功！！！\n",
      "ç¬¬ä¸ç¾åÂ·æ±å",
      "µå«å¯¨ç ´æ¹ç    æ­¦ä¾¯æéµè¾±ä»²è¾¾ 保存成功！！！\n",
      "ç¬¬ä¸ç¾åä¸åÂ·åºéä¸è¯¸èå¦ç¥    å¥åéå¼ éä¸­è®¡ 保存成功！！！\n",
      "ç¬¬ä¸ç¾åäºåÂ·å¸é©¬æ¿å ååæ¸­æ¡¥  è¯¸èäº®é æ¨çæµé©¬ 保存成功！！！\n",
      "ç¬¬ä¸ç¾åä¸åÂ·ä¸æ¹è°·å¸é©¬åå°    äºä¸åè¯¸èç¦³æ 保存成功！！！\n",
      "ç¬¬ä¸ç¾åååÂ·é¨å¤§ææ±ä¸ç¸å½å¤©  è§æ¨åé­é½ç£ä¸§è 保存成功！！！\n",
      "ç¬¬ä¸ç¾åäºåÂ·æ­¦ä¾¯é¢ä¼é¦åè®¡    é­ä¸»æåæ¿é²ç 保存成功！！！\n",
      "ç¬¬ä¸ç¾åå",
      "­åÂ·å",
      "¬å­æ¸å",
      "µè´¥æ­»è¥å¹³  å¸é©¬æ¿è¯ç",
      "èµæ¹ç½ 保存成功！！！\n",
      "ç¬¬ä¸ç¾åä¸åÂ·é­ä¸»æ¿å½å¸é©¬æ°    å§ç»´å",
      "µè´¥çå¤´å±± 保存成功！！！\n",
      "ç¬¬ä¸ç¾åå",
      "«åÂ·ä¸å¥éªä¸­å¥ç­å",
      "µ    å­å³»å¸­é´æ½å¯è®¡ 保存成功！！！\n",
      "ç¬¬ä¸ç¾åä¹åÂ·å°å¸é©¬æ±å°å¥è°    åºæ¹è³é­å®¶ææ¥ 保存成功！！！\n",
      "ç¬¬ä¸ç¾ä¸ååÂ·æé¸¯åéªééå",
      "µ    å§ç»´èæ°´ç ´å¤§æ 保存成功！！！\n",
      "ç¬¬ä¸ç¾ä¸åä¸åÂ·éå£«è½½æºè´¥å§ä¼¯çº¦  è¯¸èè¯ä¹è®¨å¸é©¬æ­ 保存成功！！！\n",
      "ç¬¬ä¸ç¾ä¸åäºåÂ·æå¯¿æ¥äºè¯ æ­»è    åé¿åä¼¯çº¦éå",
      "µ 保存成功！！！\n",
      "ç¬¬ä¸ç¾ä¸åä¸åÂ·ä¸å¥å®è®¡æ©å­ç¶    å§ç»´æéµç ´éè¾ 保存成功！！！\n",
      "ç¬¬ä¸ç¾ä¸åååÂ·æ¹é«¦é©±è½¦æ­»åé    å§ç»´å¼ç²®èé­å",
      "µ 保存成功！！！\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¬¬ä¸ç¾ä¸åäºåÂ·è¯ç­å¸åä¸»ä¿¡è°    æå±¯ç°å§ç»´é¿ç¥¸ 保存成功！！！\n",
      "ç¬¬ä¸ç¾ä¸åå",
      "­åÂ·éä¼åå",
      "µæ±ä¸­é    æ­¦ä¾¯æ¾å£å®åå±± 保存成功！！！\n",
      "ç¬¬ä¸ç¾ä¸åä¸åÂ·éå£«è½½å·åº¦é´å¹³    è¯¸èç»ææ­»ç»µç«¹ 保存成功！！！\n",
      "ç¬¬ä¸ç¾ä¸åå",
      "«åÂ·å­ç¥åºä¸çæ­»å­    å",
      "¥è¥¿å·äºå£«äºå 保存成功！！！\n",
      "ç¬¬ä¸ç¾ä¸åä¹åÂ·åæéå·§è®¡æèè¯  ååç¦",
      "ä¾æ ·ç»è«è¦ 保存成功！！！\n",
      "ç¬¬ä¸ç¾äºååÂ·èæé¢èå°ç®æ°è°  éå­çä¸åå½ä¸ç» 保存成功！！！\n"
     ]
    }
   ],
   "source": [
    "main_url = 'http://www.shicimingju.com/book/sanguoyanyi.html'\n",
    "page_text = requests.get(url=main_url,headers=headers).text\n",
    "fp = open('./sanguo.txt','w',encoding='utf-8')\n",
    "#数据解析：章节标题，详情页url，章节内容\n",
    "soup = BeautifulSoup(page_text,'lxml')\n",
    "#定位到的所有的符合要求的a标签\n",
    "a_list = soup.select('.book-mulu > ul > li > a')\n",
    "for a in a_list:\n",
    "    title = a.string\n",
    "    detail_url = 'http://www.shicimingju.com'+a['href']\n",
    "    \n",
    "    #对详情页发起请求解析出章节内容\n",
    "    page_text_detail = requests.get(url=detail_url,headers=headers).text\n",
    "    soup = BeautifulSoup(page_text_detail,'lxml')\n",
    "    div_tag = soup.find('div',class_=\"chapter_content\")\n",
    "    content = div_tag.text\n",
    "    fp.write(title+':'+content+'\\n')\n",
    "    print(title,'保存成功！！！')\n",
    "fp.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xpath解析\n",
    "- 环境安装：\n",
    "    - pip install lxml\n",
    "- 解析原理:html标签是以树状的形式进行展示\n",
    "    - 1.实例化一个etree的对象，且将待解析的页面源码数据加载到该对象中\n",
    "    - 2.调用etree对象的xpath方法结合着不同的xpath表达式实现标签的定位和数据提取\n",
    "- 实例化etree对象\n",
    "    - etree.parse('filename'):将本地html文档加载到该对象中\n",
    "    - etree.HTML(page_text):网站获取的页面数据加载到该对象\n",
    "- 标签定位：\n",
    "    - 最左侧的/:如果xpath表达式最左侧是以/开头则表示该xpath表达式一定要从根标签开始定位指定标签(忽略)\n",
    "    - 非最左侧的/:表示一个层级\n",
    "    - 非左侧的//:表示多个层级\n",
    "    - 最左侧的//：xpath表达式可以从任意位置进行标签定位\n",
    "    \n",
    "    - 属性定位：tagName[@attrName=\"value\"]\n",
    "    - 索引定位：tag[index]:索引是从1开始\n",
    "    - 模糊匹配：\n",
    "        - //div[contains(@class, \"ng\")]\n",
    "        - //div[starts-with(@class, \"ta\")]\n",
    "- 取文本\n",
    "    - /text():直系文本内容\n",
    "    - //text()：所有的文本内容\n",
    "- 取属性\n",
    "    - /@attrName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from lxml import etree\n",
    "tree = etree.parse('test.html')\n",
    "tree.xpath('/html/head/meta') #定位meta\n",
    "tree.xpath('/html//meta') #定位meta\n",
    "tree.xpath('/html//meta')#定位meta\n",
    "tree.xpath('//meta') #定位meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element p at 0x106c9aec8>,\n",
       " <Element p at 0x1071e4c08>,\n",
       " <Element p at 0x1071e4508>,\n",
       " <Element p at 0x1071e46c8>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定位class为song的div下面所有的p\n",
    "tree.xpath('//div[@class=\"song\"]/p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['李清照']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定位class为song的div下面第2个p\n",
    "tree.xpath('//div[@class=\"song\"]/p[1]//text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.haha.com']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.xpath('//a[@id=\"feng\"]/@href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用xpath爬取图片名称和图片数据\n",
    "    - http://pic.netbian.com/4kmeinv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 局部数据解析：\n",
    "    - 我们要将定位到的页面中的标签作为待解析的数据。再次使用xpath表达式解析待解析的数据。\n",
    "    - 在局部数据解析的时候，xpath表达式中要使用./的操作，./表示的就是当前的局部数据(xpath的调用者)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "糖果性感美女4k壁纸.jpg 保存成功！！！\n",
      "陆萱萱 黑色丝袜美腿 养眼好看身材4k美女壁纸.jpg 保存成功！！！\n",
      "刘奕宁Lynn 白色衣服 黑色短裙 长发清新美女4K壁纸.jpg 保存成功！！！\n",
      "居家小清新白色小吊带美女4k壁纸.jpg 保存成功！！！\n",
      "陆萱萱 制服黑色丝袜长腿美女4k壁纸.jpg 保存成功！！！\n",
      "糖果 美女模特4k壁纸.jpg 保存成功！！！\n",
      "美女模特制服连衣裙黑色丝袜长腿高跟鞋4k美女壁纸.jpg 保存成功！！！\n",
      "白色婚纱裙子 白色丝袜美腿美女4k壁纸.jpg 保存成功！！！\n",
      "收银员制服美女陆萱萱4k壁纸3840x2160.jpg 保存成功！！！\n",
      "美女陆萱萱4k壁纸3840x2160.jpg 保存成功！！！\n",
      "美脚美腿美足美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "居家 一个人 长发美女 美腿 沙发 4k美女壁纸3840x2160.jpg 保存成功！！！\n",
      "ceci美女模特4k壁纸.jpg 保存成功！！！\n",
      "西装短裙女神杨紫嫣4k壁纸.jpg 保存成功！！！\n",
      "杨紫嫣4k美女壁纸3840x2160.jpg 保存成功！！！\n",
      "成果诸葛大力4k高清壁纸3840x2160.jpg 保存成功！！！\n",
      "成果高清4k壁纸3840x2160.jpg 保存成功！！！\n",
      "居家 沙发 养眼美女 白色裙子 白色丝袜美腿4k美女壁纸3840x2160.jpg 保存成功！！！\n",
      "居家好身材养眼美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "腿模Rubis 大波浪卷发美女 连衣包臀裙美腿屁股4k壁纸3840x2160.jpg 保存成功！！！\n",
      "绿色植物 清新养眼美女4k壁纸3840x2160.jpg 保存成功！！！\n"
     ]
    }
   ],
   "source": [
    "#爬取第一页的\n",
    "dirName = 'GirlsLib'\n",
    "if not os.path.exists(dirName):\n",
    "    os.mkdir(dirName)\n",
    "\n",
    "url = 'http://pic.netbian.com/4kmeinv/'\n",
    "response = requests.get(url=url,headers=headers)\n",
    "response.encoding = 'gbk'\n",
    "page_text = response.text\n",
    "\n",
    "#图片名称+图片数据\n",
    "tree = etree.HTML(page_text)\n",
    "#存储的是定位到的指定的li标签\n",
    "li_list = tree.xpath('//div[@class=\"slist\"]/ul/li')\n",
    "for li in li_list:\n",
    "#     print(type(li)) #li的数据类型和tree的数据类型一样，li也可以调用xpath方法\n",
    "    title = li.xpath('./a/img/@alt')[0]+'.jpg'#进行局部数据解析\n",
    "    img_src = 'http://pic.netbian.com'+li.xpath('./a/img/@src')[0]\n",
    "    img_data = requests.get(url=img_src,headers=headers).content\n",
    "    imgPath = dirName +'/'+title\n",
    "    with open(imgPath,'wb') as fp:\n",
    "        fp.write(img_data)\n",
    "    print(title,'保存成功！！！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "糖果性感美女4k壁纸.jpg 保存成功！！！\n",
      "陆萱萱 黑色丝袜美腿 养眼好看身材4k美女壁纸.jpg 保存成功！！！\n",
      "刘奕宁Lynn 白色衣服 黑色短裙 长发清新美女4K壁纸.jpg 保存成功！！！\n",
      "居家小清新白色小吊带美女4k壁纸.jpg 保存成功！！！\n",
      "陆萱萱 制服黑色丝袜长腿美女4k壁纸.jpg 保存成功！！！\n",
      "糖果 美女模特4k壁纸.jpg 保存成功！！！\n",
      "美女模特制服连衣裙黑色丝袜长腿高跟鞋4k美女壁纸.jpg 保存成功！！！\n",
      "白色婚纱裙子 白色丝袜美腿美女4k壁纸.jpg 保存成功！！！\n",
      "收银员制服美女陆萱萱4k壁纸3840x2160.jpg 保存成功！！！\n",
      "美女陆萱萱4k壁纸3840x2160.jpg 保存成功！！！\n",
      "美脚美腿美足美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "居家 一个人 长发美女 美腿 沙发 4k美女壁纸3840x2160.jpg 保存成功！！！\n",
      "ceci美女模特4k壁纸.jpg 保存成功！！！\n",
      "西装短裙女神杨紫嫣4k壁纸.jpg 保存成功！！！\n",
      "杨紫嫣4k美女壁纸3840x2160.jpg 保存成功！！！\n",
      "成果诸葛大力4k高清壁纸3840x2160.jpg 保存成功！！！\n",
      "成果高清4k壁纸3840x2160.jpg 保存成功！！！\n",
      "居家 沙发 养眼美女 白色裙子 白色丝袜美腿4k美女壁纸3840x2160.jpg 保存成功！！！\n",
      "居家好身材养眼美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "腿模Rubis 大波浪卷发美女 连衣包臀裙美腿屁股4k壁纸3840x2160.jpg 保存成功！！！\n",
      "绿色植物 清新养眼美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "穿白色裙子的美女 看书 4k美女壁纸.jpg 保存成功！！！\n",
      "白色衬衫长发清纯美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "美女旅拍4k壁纸3840x2160.jpg 保存成功！！！\n",
      "许诺连衣裙美女4k高清壁纸3840x2160.jpg 保存成功！！！\n",
      "美女许诺居家写真4k高清壁纸3840x2160.jpg 保存成功！！！\n",
      "冬季闭上眼睛的美女围巾4k壁纸3840x2160.jpg 保存成功！！！\n",
      "写字的清纯美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "制服美女 美腿 慕羽茜4K美女壁纸 3840x2160.jpg 保存成功！！！\n",
      "冬季穿羽绒服清纯可爱美女 围巾 4k壁纸3840x2160.jpg 保存成功！！！\n",
      "冬天 清纯美女 围巾 4k美女壁纸3840x2160.jpg 保存成功！！！\n",
      "鲜花 美女 婚纱照 新娘 5K壁纸.jpg 保存成功！！！\n",
      "长发美女居家写真4k壁纸3840x2160.jpg 保存成功！！！\n",
      "姜璐4k壁纸.jpg 保存成功！！！\n",
      "沙发 白色内衣 性感长腿美女 刘奕宁lynn 4K壁纸.jpg 保存成功！！！\n",
      "茶园唯美古风美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "公园 黑色蕾丝打底裙美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "茶园 绿色裙子美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "干洗店 洗衣机 清纯可爱美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "张多多4K美女壁纸.jpg 保存成功！！！\n",
      "白色小吊带养眼美女4k壁纸.jpg 保存成功！！！\n",
      "可爱腿模Wendy养眼美腿4k美女壁纸.jpg 保存成功！！！\n",
      "白色睡裙美女 侧躺 书 烛光 4k美女壁纸3840x2160.jpg 保存成功！！！\n",
      "美女腿模Wendy 黑色包臀裙 黑丝美腿4k壁纸3840x2160.jpg 保存成功！！！\n",
      "可爱短发美女Wendy 蓝色裙子 白色丝袜 高跟鞋 4k美女壁纸.jpg 保存成功！！！\n",
      "腿模Wendy 红色上衣 黑色包臀裙 黑色丝袜美腿4k美女壁纸.jpg 保存成功！！！\n",
      "穿粉红色连衣裙美女3440x1440壁纸.jpg 保存成功！！！\n",
      "穿粉红色连衣裙的外国美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "美女白色婚纱裙子好身材5120x1440壁纸.jpg 保存成功！！！\n",
      "尤果网美女赵智妍4k壁纸.jpg 保存成功！！！\n",
      "腿模Wendy包臀裙黑丝袜美腿高跟鞋4k美女壁纸3840x2160.jpg 保存成功！！！\n",
      "美女Wendy包臀裙黑色丝袜美腿4k壁纸3840x2160.jpg 保存成功！！！\n",
      "腿模Wendy养眼美腿美女高跟鞋4k壁纸3840x2160.jpg 保存成功！！！\n",
      "美女腿模Wendy蓝色连衣裙4k美女壁纸3840x2160.jpg 保存成功！！！\n",
      "文艺居家养眼美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "侧躺可爱运动装美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "长发睡裙清纯美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "钢琴美女唯美4k壁纸3840x2160.jpg 保存成功！！！\n",
      "腿模Rubis水手服美腿高跟鞋4k美女壁纸.jpg 保存成功！！！\n",
      "腿模ChiChi内衣丝袜美腿写真4k壁纸.jpg 保存成功！！！\n",
      "Beautyleg腿模Rubis长发大波浪美腿丝袜4k壁纸.jpg 保存成功！！！\n",
      "美女腿模Rubis包臀裙4k美女壁纸3840x2160.jpg 保存成功！！！\n",
      "Wendy旗袍美女美腿4k壁纸3840x2160.jpg 保存成功！！！\n",
      "居家 小清新 可爱清纯闭眼美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "腿模Wendy水手服养眼美腿4k壁纸3840x2160.jpg 保存成功！！！\n",
      "腿模Wendy黑色丝袜美腿4k美女壁纸3840x2160.jpg 保存成功！！！\n",
      "腿模Wendy旗袍美腿4k壁纸.jpg 保存成功！！！\n",
      "Emma腿模 水手服 白色丝袜美腿4k壁纸3840x2160.jpg 保存成功！！！\n",
      "腿模Yun水手服美腿4k美女壁纸.jpg 保存成功！！！\n",
      "腿模Leonie美腿 白色衬衫 黑色短裙4k美女壁纸.jpg 保存成功！！！\n",
      "腿模Leonie旗袍美女美腿4k壁纸.jpg 保存成功！！！\n",
      "腿模Leonie养眼美腿4k美女壁纸.jpg 保存成功！！！\n",
      "Emma 旗袍美女美腿黑色丝袜4k美女壁纸3840x2160.jpg 保存成功！！！\n",
      "黑色长发腿模Emma 紫色衬衫 白色短裙 美腿美女4k壁纸.jpg 保存成功！！！\n",
      "腿模Emma 红色吊带裙养眼美腿美女4k壁纸.jpg 保存成功！！！\n",
      "美女腿模Emma旗袍黑丝美腿4k壁纸.jpg 保存成功！！！\n",
      "晚上天台可爱养眼美女4k壁纸.jpg 保存成功！！！\n",
      "居家看书的美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "居家 沙发 小清新长发美女 白色裙子 唯美美女4k壁纸.jpg 保存成功！！！\n",
      "居家文艺美女4k壁纸.jpg 保存成功！！！\n",
      "剪刀手势 v字手势可爱美女4k壁纸.jpg 保存成功！！！\n",
      "黑色内衣文胸美女王诗琪4k壁纸3840x2160.jpg 保存成功！！！\n",
      "居家小清新清纯美女5120x1440壁纸.jpg 保存成功！！！\n",
      "趴在桌子上睡觉的美女4k壁纸.jpg 保存成功！！！\n",
      "白色睡衣养眼美女王诗琪4k壁纸3840x2160.jpg 保存成功！！！\n",
      "王诗琪4k壁纸.jpg 保存成功！！！\n",
      "居家内衣美女模特晓晓4k壁纸.jpg 保存成功！！！\n",
      "海边清纯美女 帽子 白色裙子 4k壁纸.jpg 保存成功！！！\n",
      "可爱美女坐在沙滩听音乐4k壁纸.jpg 保存成功！！！\n",
      "克拉女神-芊芊《夕阳照红颜》4k美女壁纸.jpg 保存成功！！！\n",
      "克拉女神-周拉拉《暗香蕾丝》4k壁纸.jpg 保存成功！！！\n",
      "居家白色睡衣小清新养眼美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "凌雪 性感空姐美女屁股4k壁纸.jpg 保存成功！！！\n",
      "比基尼性感美女于思琪4k壁纸.jpg 保存成功！！！\n",
      "海滩比基尼长发美女4k壁纸.jpg 保存成功！！！\n",
      "侧脸香肩唯美美女4k壁纸.jpg 保存成功！！！\n",
      "克拉女神-蓓颖 风度娴雅 4k美女壁纸.jpg 保存成功！！！\n",
      "美替3440x1440美女壁纸.jpg 保存成功！！！\n",
      "美女美替4k壁纸.jpg 保存成功！！！\n",
      "女神美替4k壁纸.jpg 保存成功！！！\n",
      "呼吸小木屋里阳光的味道 妮妮摄影4K壁纸.jpg 保存成功！！！\n",
      "平躺在床上的小清新美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "猫宝 粉色内衣 皮毛 性感身材美女4k壁纸3840x2160.jpg 保存成功！！！\n",
      "海边比基尼性感身材美女4k壁纸.jpg 保存成功！！！\n",
      "克拉女神芊芊 夕阳照红颜 4k美女壁纸.jpg 保存成功！！！\n"
     ]
    }
   ],
   "source": [
    "#爬取多页\n",
    "dirName = 'GirlsLib'\n",
    "if not os.path.exists(dirName):\n",
    "    os.mkdir(dirName)\n",
    "    \n",
    "#定义一个通用的url模板:不可变\n",
    "url = 'http://pic.netbian.com/4kmeinv/index_%d.html'\n",
    "for page in range(1,6):\n",
    "    if page == 1:\n",
    "        new_url = 'http://pic.netbian.com/4kmeinv/'\n",
    "    else:\n",
    "        new_url = format(url%page)\n",
    "    response = requests.get(url=new_url,headers=headers)\n",
    "    response.encoding = 'gbk'\n",
    "    page_text = response.text\n",
    "\n",
    "    #图片名称+图片数据\n",
    "    tree = etree.HTML(page_text)\n",
    "    #存储的是定位到的指定的li标签\n",
    "    li_list = tree.xpath('//div[@class=\"slist\"]/ul/li')\n",
    "    for li in li_list:\n",
    "    #     print(type(li)) #li的数据类型和tree的数据类型一样，li也可以调用xpath方法\n",
    "        title = li.xpath('./a/img/@alt')[0]+'.jpg'#进行局部数据解析\n",
    "        img_src = 'http://pic.netbian.com'+li.xpath('./a/img/@src')[0]\n",
    "        img_data = requests.get(url=img_src,headers=headers).content\n",
    "        imgPath = dirName +'/'+title\n",
    "        with open(imgPath,'wb') as fp:\n",
    "            fp.write(img_data)\n",
    "        print(title,'保存成功！！！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 需求：要求解析出携带html标签的局部数据？\n",
    "    - bs4,bs4在实现标签定位的时候返回的直接就是定位到标签对应的字符串数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xpath表达式如何更加具有通用性？\n",
    "    - 在xpath表达式中使用管道符分割的作用，可以表示管道符左右两侧的子xpath表达式同时生效或者一个生效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['北京',\n",
       " '上海',\n",
       " '广州',\n",
       " '深圳',\n",
       " '杭州',\n",
       " '天津',\n",
       " '成都',\n",
       " '南京',\n",
       " '西安',\n",
       " '武汉',\n",
       " '阿坝州',\n",
       " '安康',\n",
       " '阿克苏地区',\n",
       " '阿里地区',\n",
       " '阿拉善盟',\n",
       " '阿勒泰地区',\n",
       " '安庆',\n",
       " '安顺',\n",
       " '鞍山',\n",
       " '克孜勒苏州',\n",
       " '安阳',\n",
       " '蚌埠',\n",
       " '白城',\n",
       " '保定',\n",
       " '北海',\n",
       " '宝鸡',\n",
       " '北京',\n",
       " '毕节',\n",
       " '博州',\n",
       " '百色',\n",
       " '白沙',\n",
       " '白山',\n",
       " '保山',\n",
       " '保亭',\n",
       " '包头',\n",
       " '本溪',\n",
       " '白银',\n",
       " '巴彦淖尔',\n",
       " '滨州',\n",
       " '巴中',\n",
       " '亳州',\n",
       " '长春',\n",
       " '承德',\n",
       " '成都',\n",
       " '常德',\n",
       " '昌都',\n",
       " '赤峰',\n",
       " '昌江',\n",
       " '昌吉州',\n",
       " '五家渠',\n",
       " '澄迈',\n",
       " '重庆',\n",
       " '常熟',\n",
       " '长沙',\n",
       " '楚雄州',\n",
       " '朝阳',\n",
       " '滁州',\n",
       " '郴州',\n",
       " '潮州',\n",
       " '常州',\n",
       " '长治',\n",
       " '崇左',\n",
       " '沧州',\n",
       " '池州',\n",
       " '定安',\n",
       " '丹东',\n",
       " '东方',\n",
       " '东莞',\n",
       " '德宏州',\n",
       " '大连',\n",
       " '大理州',\n",
       " '大庆',\n",
       " '大同',\n",
       " '定西',\n",
       " '大兴安岭地区',\n",
       " '黔南州',\n",
       " '德阳',\n",
       " '东营',\n",
       " '达州',\n",
       " '德州',\n",
       " '儋州',\n",
       " '鄂尔多斯',\n",
       " '恩施州',\n",
       " '鄂州',\n",
       " '防城港',\n",
       " '抚顺',\n",
       " '佛山',\n",
       " '阜新',\n",
       " '阜阳',\n",
       " '富阳',\n",
       " '福州',\n",
       " '抚州',\n",
       " '广安',\n",
       " '贵港',\n",
       " '果洛州',\n",
       " '桂林',\n",
       " '甘南州',\n",
       " '贵阳',\n",
       " '广元',\n",
       " '固原',\n",
       " '广州',\n",
       " '甘孜州',\n",
       " '赣州',\n",
       " '淮安',\n",
       " '淮北',\n",
       " '鹤壁',\n",
       " '海北州',\n",
       " '河池',\n",
       " '邯郸',\n",
       " '海东地区',\n",
       " '哈尔滨',\n",
       " '合肥',\n",
       " '黄冈',\n",
       " '鹤岗',\n",
       " '红河州',\n",
       " '怀化',\n",
       " '黑河',\n",
       " '呼和浩特',\n",
       " '海口',\n",
       " '呼伦贝尔',\n",
       " '葫芦岛',\n",
       " '海门',\n",
       " '哈密地区',\n",
       " '淮南',\n",
       " '黄南州',\n",
       " '海南州',\n",
       " '黄山',\n",
       " '衡水',\n",
       " '黄石',\n",
       " '和田地区',\n",
       " '海西州',\n",
       " '衡阳',\n",
       " '河源',\n",
       " '湖州',\n",
       " '汉中',\n",
       " '杭州',\n",
       " '贺州',\n",
       " '菏泽',\n",
       " '惠州',\n",
       " '吉安',\n",
       " '金昌',\n",
       " '晋城',\n",
       " '景德镇',\n",
       " '西双版纳州',\n",
       " '金华',\n",
       " '九江',\n",
       " '吉林',\n",
       " '荆门',\n",
       " '江门',\n",
       " '即墨',\n",
       " '佳木斯',\n",
       " '济南',\n",
       " '济宁',\n",
       " '胶南',\n",
       " '酒泉',\n",
       " '句容',\n",
       " '湘西州',\n",
       " '金坛',\n",
       " '嘉兴',\n",
       " '鸡西',\n",
       " '济源',\n",
       " '揭阳',\n",
       " '江阴',\n",
       " '嘉峪关',\n",
       " '锦州',\n",
       " '荆州',\n",
       " '晋中',\n",
       " '焦作',\n",
       " '胶州',\n",
       " '库尔勒',\n",
       " '开封',\n",
       " '黔东南州',\n",
       " '克拉玛依',\n",
       " '昆明',\n",
       " '昆山',\n",
       " '喀什地区',\n",
       " '临安',\n",
       " '六安',\n",
       " '来宾',\n",
       " '聊城',\n",
       " '临沧',\n",
       " '乐东',\n",
       " '娄底',\n",
       " '廊坊',\n",
       " '临汾',\n",
       " '临高',\n",
       " '漯河',\n",
       " '丽江',\n",
       " '吕梁',\n",
       " '陇南',\n",
       " '六盘水',\n",
       " '丽水',\n",
       " '凉山州',\n",
       " '拉萨',\n",
       " '乐山',\n",
       " '陵水',\n",
       " '莱芜',\n",
       " '临夏州',\n",
       " '莱西',\n",
       " '辽源',\n",
       " '辽阳',\n",
       " '溧阳',\n",
       " '龙岩',\n",
       " '洛阳',\n",
       " '临沂',\n",
       " '连云港',\n",
       " '莱州',\n",
       " '林芝',\n",
       " '泸州',\n",
       " '柳州',\n",
       " '兰州',\n",
       " '马鞍山',\n",
       " '牡丹江',\n",
       " '茂名',\n",
       " '眉山',\n",
       " '绵阳',\n",
       " '梅州',\n",
       " '宁波',\n",
       " '南充',\n",
       " '南昌',\n",
       " '宁德',\n",
       " '南京',\n",
       " '内江',\n",
       " '怒江州',\n",
       " '南宁',\n",
       " '南平',\n",
       " '那曲地区',\n",
       " '南通',\n",
       " '南阳',\n",
       " '平度',\n",
       " '平顶山',\n",
       " '普洱',\n",
       " '盘锦',\n",
       " '蓬莱',\n",
       " '平凉',\n",
       " '莆田',\n",
       " '萍乡',\n",
       " '濮阳',\n",
       " '攀枝花',\n",
       " '青岛',\n",
       " '琼海',\n",
       " '秦皇岛',\n",
       " '曲靖',\n",
       " '齐齐哈尔',\n",
       " '七台河',\n",
       " '黔西南州',\n",
       " '清远',\n",
       " '庆阳',\n",
       " '钦州',\n",
       " '衢州',\n",
       " '琼中',\n",
       " '泉州',\n",
       " '荣成',\n",
       " '日喀则',\n",
       " '乳山',\n",
       " '日照',\n",
       " '寿光',\n",
       " '韶关',\n",
       " '上海',\n",
       " '绥化',\n",
       " '石河子',\n",
       " '石家庄',\n",
       " '商洛',\n",
       " '三明',\n",
       " '三门峡',\n",
       " '遂宁',\n",
       " '山南',\n",
       " '四平',\n",
       " '宿迁',\n",
       " '商丘',\n",
       " '上饶',\n",
       " '汕头',\n",
       " '汕尾',\n",
       " '绍兴',\n",
       " '松原',\n",
       " '沈阳',\n",
       " '十堰',\n",
       " '三亚',\n",
       " '邵阳',\n",
       " '双鸭山',\n",
       " '朔州',\n",
       " '苏州',\n",
       " '宿州',\n",
       " '随州',\n",
       " '深圳',\n",
       " '石嘴山',\n",
       " '泰安',\n",
       " '铜川',\n",
       " '屯昌',\n",
       " '太仓',\n",
       " '塔城地区',\n",
       " '通化',\n",
       " '天津',\n",
       " '铁岭',\n",
       " '铜陵',\n",
       " '通辽',\n",
       " '吐鲁番地区',\n",
       " '铜仁地区',\n",
       " '唐山',\n",
       " '天水',\n",
       " '太原',\n",
       " '台州',\n",
       " '泰州',\n",
       " '文昌',\n",
       " '文登',\n",
       " '潍坊',\n",
       " '瓦房店',\n",
       " '武汉',\n",
       " '乌海',\n",
       " '芜湖',\n",
       " '威海',\n",
       " '吴江',\n",
       " '乌兰察布',\n",
       " '乌鲁木齐',\n",
       " '渭南',\n",
       " '万宁',\n",
       " '文山州',\n",
       " '武威',\n",
       " '无锡',\n",
       " '温州',\n",
       " '梧州',\n",
       " '吴忠',\n",
       " '五指山',\n",
       " '兴安盟',\n",
       " '西安',\n",
       " '宣城',\n",
       " '许昌',\n",
       " '襄阳',\n",
       " '孝感',\n",
       " '迪庆州',\n",
       " '锡林郭勒盟',\n",
       " '厦门',\n",
       " '西宁',\n",
       " '咸宁',\n",
       " '湘潭',\n",
       " '邢台',\n",
       " '新乡',\n",
       " '咸阳',\n",
       " '新余',\n",
       " '信阳',\n",
       " '忻州',\n",
       " '徐州',\n",
       " '雅安',\n",
       " '延安',\n",
       " '延边州',\n",
       " '宜宾',\n",
       " '伊春',\n",
       " '银川',\n",
       " '宜春',\n",
       " '宜昌',\n",
       " '盐城',\n",
       " '运城',\n",
       " '云浮',\n",
       " '阳江',\n",
       " '营口',\n",
       " '玉林',\n",
       " '榆林',\n",
       " '伊犁哈萨克州',\n",
       " '阳泉',\n",
       " '玉树州',\n",
       " '烟台',\n",
       " '鹰潭',\n",
       " '义乌',\n",
       " '宜兴',\n",
       " '玉溪',\n",
       " '益阳',\n",
       " '岳阳',\n",
       " '永州',\n",
       " '扬州',\n",
       " '淄博',\n",
       " '自贡',\n",
       " '珠海',\n",
       " '镇江',\n",
       " '湛江',\n",
       " '诸暨',\n",
       " '张家港',\n",
       " '张家界',\n",
       " '张家口',\n",
       " '周口',\n",
       " '驻马店',\n",
       " '章丘',\n",
       " '肇庆',\n",
       " '舟山',\n",
       " '中山',\n",
       " '昭通',\n",
       " '中卫',\n",
       " '招远',\n",
       " '资阳',\n",
       " '张掖',\n",
       " '遵义',\n",
       " '郑州',\n",
       " '漳州',\n",
       " '株洲',\n",
       " '枣庄']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将https://www.aqistudy.cn/historydata/所有的城市名称解析出来\n",
    "url = 'https://www.aqistudy.cn/historydata/'\n",
    "page_text = requests.get(url=url,headers=headers).text\n",
    "\n",
    "tree = etree.HTML(page_text)\n",
    "# hot_cities = tree.xpath('//div[@class=\"bottom\"]/ul/li/a/text()')\n",
    "# all_cities = tree.xpath('//div[@class=\"bottom\"]/ul/div[2]/li/a/text()')\n",
    "tree.xpath('//div[@class=\"bottom\"]/ul/li/a/text() | //div[@class=\"bottom\"]/ul/div[2]/li/a/text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
